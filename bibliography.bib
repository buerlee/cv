@article{schrimpf2017,          
title={A Flexible Approach to Automated RNN Architecture Generation},          
author={Schrimpf, Martin and Merity, Stephen and Socher, Richard},          
journal={submitted},          
year={2017}      
}

@article{cheney2017robustness,
  title={On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations},
  author={Nicholas Cheney* and Martin Schrimpf* and Gabriel Kreiman},
  journal={arXiv preprint},
  archivePrefix = "arXiv",
  eprint =       {1703.08245}, 
  primaryClass = "cs.AI",
  url =          "https://arxiv.org/abs/1703.08245",
  year={2017},
  abstract = "Deep convolutional neural networks are generally regarded as robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust to a number of internal perturbations in the higher convolutional layers but the bottom convolutional layers are much more fragile. For instance, Alexnet shows less than a 30% decrease in classification performance when randomly removing over 70% of weight connections in the top convolutional or dense layers but performance is almost at chance with the same perturbation in the first convolutional layer. Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations."
}

@article{recurrency_occlusion,
    author =       "Hanlin Tang* and Martin Schrimpf* and Bill Lotter* and Charlotte Moerman and Ana Paredes and Josue Ortega Caro and Walter Hardesty and David Cox and Gabriel Kreiman",
    title =        "Recurrent computations for pattern completion",
    journal =      "submitted",
    pubtype =      "", 
    year =         "2017", 
    abstract =     "Making inferences from partial information constitutes a critical aspect of cognition. In visual perception, pattern completion leads to recognition of poorly visible or occluded objects. We combined psychophysics, physiology and computational models to test the hypothesis that pattern completion is implemented by recurrent computations. First, human subjects robustly recognized objects even when rendered <10% visible, but performance deteriorated when recurrent computations were interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex also exhibited pattern completion, where responses to partially visible objects were delayed compared to whole objects, suggesting the need for additional computations. These physiological delays were correlated with the backward masking effects. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, a model augmented with attractor-based recurrent connectivity matched human performance. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information."
}

@manual{should_i_use_tensorflow,
    author =       "Martin Schrimpf",
    title =        "Should I use Tensorflow",
    note =         "Seminar Paper", 
    organization = "University of Augsburg", 
    year =         "2016",
    archivePrefix = "arXiv",
    eprint =       {1611.08903}, 
    primaryClass = "cs.AI",
    url =          "https://arxiv.org/abs/1611.08903",
    abstract =     "Google's Machine Learning framework TensorFlow was open-sourced in November 2015 and has since built a growing community around it. TensorFlow is supposed to be flexible for research purposes while also allowing its models to be deployed productively. This work is aimed towards people with experience in Machine Learning considering whether they should use TensorFlow in their environment. Several aspects of the framework important for such a decision are examined, such as the heterogenity, extensibility and its computation graph. A pure Python implementation of linear classification is compared with an implementation utilizing TensorFlow. I also contrast TensorFlow to other popular frameworks with respect to modeling capability, deployment and performance and give a brief description of the current adaption of the framework."
}

@MastersThesis{bachelors_thesis,
    type =         "Bachelor's Thesis", 
    author =       "Martin Schrimpf",
    title =        "Scalable Database Concurrency Control using Transactional Memory",
    school =       "Technical University Munich",
    year =         "2014",
    url =          "http://mschrimpf.com/wp-content/uploads/2016/11/Scalable-Database-Concurrency-Control-using-Transactional-Memory-Martin-Schrimpf-TextSigned.pdf",
    abstract =     "Intel recently made available the optimistic synchronization technique Hardware Transactional
Memory (HTM) in their mainstream Haswell processor microarchitecture.
The first part of this work evaluates the core performance characteristics of the two programming
interfaces within Intel’s Transactional Synchronization Extensions (TSX), Hardware Lock Elision
(HLE) and Restricted Transactional Memory (RTM). Therein, a scope of application is defined
regarding inter alia the transaction size which is limited to the L1 DCache or even less with wrongly
aligned data due to cache associativity, the transaction duration restricted by Hardware interrupts
and a limit to the nesting of transactions. By comparing common data structures and analyzing
the behavior of HTM using hardware counters, the Hashmap is identified as a suitable structure
with a 134% speedup compared to classical POSIX mutexes.
In the second part, several latching mechanisms of MySQL InnoDB’s Concurrency Control are
selected and modified with different implementations of HTM to achieve increased scalability. We
find that it does not suffice to apply HTM naively to all mutex calls by using either HLE prefixes or
an HTM-enabled glibc. Furthermore, many transactional cycles often come at the price of frequent
aborted cycles which inhibits performance increases when measuring MySQL with the tx-bench
and too many aborts can even decrease the throughput to 29% of the unmodified version."
}
